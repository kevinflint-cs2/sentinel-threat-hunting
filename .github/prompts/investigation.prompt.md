# CSV Security Investigation & MITRE ATT&CK Mapping – Copilot Instructions

You are helping build and use a **Python-based security investigation toolkit** that analyzes CSV data exported from logs (e.g., process, file, network, email, Sysmon, Defender, Sentinel KQL output) and generates **markdown reports** with strong **MITRE ATT&CK** coverage.

## Project & Environment

- The repository uses **Python** as the primary language.
- A `.env` file in the **repo root** defines an environment variable: `INVESTIGATION_PATH`.
- `INVESTIGATION_PATH` points to a **single investigation folder** with this structure (create it if missing):

  - `results/` – KQL and log outputs as CSV files (input for analysis).
  - `reports/` – markdown reports generated by the AI/tool.
  - `baselines/` – (optional / evolving) CSV or other artifacts representing *known-good* or *expected* activity to assist in classifying unknowns as benign.
  - `notes/` – general analyst notes in markdown.
  - `config.yaml` – configuration file for the investigation (e.g., paths, tags, investigation metadata, severity thresholds, MITRE options).

- By default, assume:
  - CSVs are UTF-8, comma-delimited.
  - Pandas is available for CSV loading and manipulation.
  - PyYAML (or similar) is available for reading `config.yaml`.
  - The script will run as a CLI-style Python program.

When writing code, **always**:
- Read `INVESTIGATION_PATH` from the environment (e.g., `os.environ["INVESTIGATION_PATH"]`).
- Fail clearly if it’s missing (helpful error message).
- Build paths under that directory: `results`, `reports`, `baselines`, `notes`, `config.yaml`.
- Create directories if they don’t exist.

## High-Level Behavior

Implement and maintain a Python workflow that:

1. **Loads configuration**
   - Read `config.yaml` from `INVESTIGATION_PATH`.
   - Use it to determine:
     - The location of the `results` folder (allow override).
     - Any global investigation metadata (case ID, timeframe, target host(s), etc.).
     - Optional settings for MITRE mapping, thresholds, and baseline usage.

2. **Discovers and iterates CSVs**
   - Enumerate all `*.csv` files under the `results/` directory.
   - For each CSV:
     - Load into a Pandas DataFrame.
     - Automatically infer what the CSV represents based on columns (e.g., process list, file activity, network connections, email logs, startup folder artifacts).
     - Run a **security-focused analysis** and generate a **markdown report** under `reports/` with a meaningful filename (e.g., `<original-name>_report.md`).

3. **Use baselines when available**
   - If appropriate baseline files exist under `baselines/`, load them and:
     - Treat entries matching known-good patterns as **benign** unless strong indicators suggest otherwise.
     - Highlight deviations from baselines as **suspicious** or **unknown**.
   - Design the code so that the absence of baselines does **not** break analysis—it should degrade gracefully.

## Analysis Behavior for Each CSV

For each CSV (each DataFrame), perform **row-by-row, context-aware analysis**:

1. **Classification per row**
   - Classify each row as:
     - `benign`
     - `suspicious`
     - `unknown`
   - Consider:
     - Process names, command lines, parent-child relationships.
     - Paths (e.g., startup folders, temp directories, system directories).
     - Network destinations, ports, protocols.
     - File operations (create, modify, delete, rename).
     - User accounts, service names, scheduled tasks, persistence mechanisms.
     - Any other domain-specific fields present in the CSV.
   - Use baselines (if present) to reduce noise by marking clearly repetitive known-good behavior as `benign`.

2. **MITRE ATT&CK mapping (CRITICAL)**
   - For **every suspicious or clearly malicious pattern**, attempt to map activity to **MITRE ATT&CK techniques**.
   - When mapping:
     - Include **technique ID** (e.g., `T1059.003`), **tactic** (e.g., `Execution`), and **technique name**.
     - Provide a **short justification** explaining *why* the activity matches that technique.
     - When reasonable, link to the official ATT&CK page on `https://attack.mitre.org` for that technique. For example: `https://attack.mitre.org/techniques/T1059/003/`.  
     - If multiple techniques could apply, list the most relevant ones and briefly explain the trade-offs.
   - Be conservative:
     - Do **not** over-map; prefer fewer, high-confidence mappings with clear reasoning.
     - When uncertain, mark the mapping as tentative and explain the ambiguity.
   - Use knowledge from the official MITRE ATT&CK Enterprise matrix and related documentation when reasoning about mappings.

3. **Contextual reasoning**
   - Consider:
     - Time ordering of events.
     - Repeated behavior by the same process, user, or host.
     - Chains like: initial access → execution → persistence → defense evasion → exfiltration.
   - Look for patterns such as:
     - File creations in startup or run keys (persistence).
     - Suspicious command-line usage (e.g., scripting, LOLBins, encoded commands).
     - Process injection, credential dumping, lateral movement, data staging, exfil.
   - Group related suspicious rows into **logical incidents or activity clusters** where helpful.

## Markdown Report Structure (Per CSV)

For each CSV, generate a structured **markdown** report with the following sections:

1. `# <Title for this dataset>`
   - Example: `# File Creations in Startup Folder (Persistence)` or `# Suspicious Process Activity - Host XYZ`.

2. `## Summary`
   - High-level overview intended for an analyst or technical manager.
   - Include:
     - Total rows analyzed.
     - Count of rows by classification:
       - `benign`
       - `suspicious`
       - `unknown`
     - Overall assessment of the dataset (e.g., “primarily benign with a small number of suspicious persistence artifacts”).
   - **MITRE ATT&CK roll-up summary:**
     - A brief table or bullet list summarizing observed techniques by **tactic**:
       - e.g., “Execution: 3 techniques”, “Persistence: 2 techniques”, “Defense Evasion: 1 technique”.
     - Highlight any techniques of high concern.

3. `## Timeline`
   - Chronological view of **major events** from the CSV (based on time column(s) if present).
   - Focus on:
     - Suspicious or notable events.
     - Activity clusters (e.g., “Initial process launch”, “persistence setup”, “network beacons”).
   - Use markdown lists or tables; keep it readable.

4. `## Detections (KQL Queries)`
   - Provide **KQL detection examples** that could be used in Microsoft Sentinel (or similar) to detect **similar activity** in the future.
   - For each detection:
     - Give it a short name.
     - Provide the KQL query.
     - Briefly note what it looks for and which MITRE technique(s) it aligns with.
   - Ensure queries are syntactically valid, use meaningful column names, and are tuned to reduce obvious noise while still catching the behavior.

5. `## Recommendations`
   - Provide **clear, concise recommendations** for:
     - Additional data to collect or queries to run.
     - Host or account triage steps (e.g., review processes, isolate host, reset credentials).
     - Hardening and prevention (e.g., disabling unnecessary services, improving email filtering, application control).
   - Link recommendations back to MITRE ATT&CK mitigations where helpful.

6. `## Suspicious Activity Details`
   - A detailed list (table or bullet list) of all rows classified as `suspicious` (and any `unknown` that deserve attention).
   - For each item, include:
     - Key fields (timestamp, host, user, process, path, command line, network endpoint, etc.).
     - Classification (`suspicious` / `unknown`).
     - Associated MITRE techniques (IDs, names, and tactics).
     - Short explanation: *why* it is suspicious and what it might represent operationally.
   - Group related rows if they are clearly part of the same activity.

7. (Optional) `## Benign Pattern Observations`
   - Briefly note any interesting but benign patterns learned from the data (potential input to `baselines/`).

## Coding Style & Quality Expectations

When Copilot generates or edits code for this project:

- Use **Python 3.x**, `pathlib`, and clear function decomposition.
- Prefer:
  - `pandas` for CSV handling.
  - `PyYAML` or `ruamel.yaml` for YAML.
  - `python-dotenv` (if needed) or `os.environ` to read `.env`.
- Add docstrings and type hints for non-trivial functions.
- Separate:
  - Core analysis logic (working on DataFrames and returning structured analysis results).
  - Report generation (converting analysis results into markdown).
  - I/O (reading/writing files, reading config, locating directories).
- Make it easy to:
  - Plug in different CSV schemas.
  - Extend analysis for new log types.
  - Add future integrations (e.g., consuming official MITRE ATT&CK STIX data or libraries).

## When Analyzing CSVs in Chat

When the user pastes or references a CSV (or its schema) in chat and asks for analysis:

- Apply the **same analysis model** described above.
- Explicitly:
  - Classify events.
  - Map to MITRE ATT&CK with IDs, tactics, names, and reasoning.
  - Propose KQL detections.
  - Provide a markdown report or report outline matching the structure above.

If something is ambiguous or critical context is missing, **state assumptions explicitly** and proceed with the best, conservative security interpretation.

MITRE ATT&CK mapping is a **top priority**: always give it careful thought and leverage up-to-date knowledge from the official ATT&CK Enterprise matrix and documentation when reasoning about techniques.
